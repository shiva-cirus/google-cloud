/*
 * Copyright Â© 2019 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package io.cdap.plugin.gcp.firestore.sink;

import com.google.cloud.firestore.Firestore;
import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import io.cdap.cdap.api.annotation.Description;
import io.cdap.cdap.api.annotation.Macro;
import io.cdap.cdap.api.annotation.Name;
import io.cdap.cdap.api.data.schema.Schema;
import io.cdap.cdap.api.plugin.PluginConfig;
import io.cdap.cdap.etl.api.FailureCollector;
import io.cdap.plugin.gcp.common.GCPReferenceSinkConfig;
import io.cdap.plugin.gcp.firestore.sink.util.SinkIdType;
import io.cdap.plugin.gcp.firestore.util.FirestoreUtil;
import io.cdap.plugin.gcp.firestore.util.Util;

import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.stream.Collectors;
import java.util.stream.Stream;
import javax.annotation.Nullable;

import static io.cdap.plugin.gcp.firestore.sink.util.FirestoreSinkConstants.MAX_BATCH_SIZE;
import static io.cdap.plugin.gcp.firestore.sink.util.FirestoreSinkConstants.PROPERTY_BATCH_SIZE;
import static io.cdap.plugin.gcp.firestore.sink.util.FirestoreSinkConstants.PROPERTY_ID_ALIAS;
import static io.cdap.plugin.gcp.firestore.sink.util.FirestoreSinkConstants.PROPERTY_ID_TYPE;
import static io.cdap.plugin.gcp.firestore.sink.util.SinkIdType.AUTO_GENERATED_ID;
import static io.cdap.plugin.gcp.firestore.util.FirestoreConstants.ID_PROPERTY_NAME;
import static io.cdap.plugin.gcp.firestore.util.FirestoreConstants.PROPERTY_COLLECTION;
import static io.cdap.plugin.gcp.firestore.util.FirestoreConstants.PROPERTY_DATABASE_ID;

/**
 * Defines a base {@link PluginConfig} that Firestore Source and Sink can re-use.
 */
public class FirestoreSinkConfig extends GCPReferenceSinkConfig {

  @Name(PROPERTY_DATABASE_ID)
  @Description("Firestore database name.")
  @Macro
  @Nullable
  private String database;

  @Name(PROPERTY_COLLECTION)
  @Description("Name of the database collection. If the collection name does not exist in Firestore " +
    "then it will create a new collection and then the data will be written to it.")
  @Macro
  private String collection;

  @Name(PROPERTY_ID_TYPE)
  @Macro
  @Description("Type of id assigned to documents written to the Firestore. The type can be one of two values: "
    + "`Auto-generated id` - id will be auto-generated as a Alpha-numeric ID, `Custom name` - id "
    + "will be provided as a field in the input records. The id field must not be nullable and must be "
    + "of type STRING.")
  private String idType;

  @Name(PROPERTY_ID_ALIAS)
  @Macro
  @Nullable
  @Description("The field that will be used as the document id when writing to Cloud Firestore. This must be provided "
    + "when the Id Type is not auto generated.")
  private String idAlias;

  @Name(PROPERTY_BATCH_SIZE)
  @Macro
  @Description("Maximum number of documents that can be passed in one batch to a Commit operation. "
    + "The minimum value is 1 and maximum value is 500")
  private int batchSize;

  public FirestoreSinkConfig() {
    // needed for initialization
  }

  @VisibleForTesting
  public FirestoreSinkConfig(String referenceName, String project, String serviceFilePath, String database,
                             String collection, String idType, String idAlias, int batchSize) {
    this.referenceName = referenceName;
    this.project = project;
    this.serviceFilePath = serviceFilePath;
    this.database = database;
    this.collection = collection;
    this.idType = idType;
    this.idAlias = idAlias;
    this.batchSize = batchSize;
  }

  public String getReferenceName() {
    return referenceName;
  }

  @Nullable
  public String getDatabase() {
    return database;
  }

  public String getCollection() {
    return collection;
  }

  public SinkIdType getIdType(FailureCollector collector) {
    Optional<SinkIdType> sinkIdType = SinkIdType.fromValue(idType);
    if (sinkIdType.isPresent()) {
      return sinkIdType.get();
    }
    collector.addFailure("Unsupported id type value: " + idType,
      String.format("Supported types are: %s", SinkIdType.getSupportedTypes()))
      .withConfigProperty(PROPERTY_ID_TYPE);
    throw collector.getOrThrowException();
  }

  @Nullable
  public String getIdAlias() {
    return Util.isNullOrEmpty(idAlias) ? ID_PROPERTY_NAME : idAlias;
  }

  public int getBatchSize() {
    return batchSize;
  }

  public boolean shouldUseAutoGeneratedId(FailureCollector collector) {
    return getIdType(collector) == AUTO_GENERATED_ID;
  }

  /**
   * Validates {@link FirestoreSinkConfig} instance.
   */
  public void validate(@Nullable Schema schema, FailureCollector collector) {
    super.validate(collector);

    validateBatchSize(collector);
    validateFirestoreConnection(collector);

    if (schema != null) {
      validateSchema(schema, collector);
      validateIdType(schema, collector);
    }
  }

  @VisibleForTesting
  void validateFirestoreConnection(FailureCollector collector) {
    if (!shouldConnect()) {
      return;
    }
    try {
      Firestore db = FirestoreUtil.getFirestore(getServiceAccountFilePath(), getProject(), getDatabase());
      db.close();
    } catch (Exception e) {
      collector.addFailure(e.getMessage(), "Ensure properties like project, service account " +
        "file path are correct.")
        .withConfigProperty(NAME_SERVICE_ACCOUNT_FILE_PATH)
        .withConfigProperty(NAME_PROJECT)
        .withStacktrace(e.getStackTrace());
    }
  }

  private void validateSchema(Schema schema, FailureCollector collector) {
    List<Schema.Field> fields = schema.getFields();
    if (fields == null || fields.isEmpty()) {
      collector.addFailure("Sink schema must contain at least one field", null);
    } else {
      fields.forEach(f -> validateSinkFieldSchema(f.getName(), f.getSchema(), collector));
    }
  }

  /**
   * Validates given field schema to be complaint with Firestore types.
   * Will throw {@link IllegalArgumentException} if schema contains unsupported type.
   *
   * @param fieldName   field name
   * @param fieldSchema schema for CDAP field
   * @param collector   failure collector
   */
  private void validateSinkFieldSchema(String fieldName, Schema fieldSchema, FailureCollector collector) {
    Schema.LogicalType logicalType = fieldSchema.getLogicalType();
    if (logicalType != null) {
      switch (logicalType) {
        // timestamps in CDAP are represented as LONG with TIMESTAMP_MICROS logical type
        case TIMESTAMP_MICROS:
        case TIMESTAMP_MILLIS:
          break;
        default:
          collector.addFailure(String.format("Field '%s' is of unsupported type '%s'",
            fieldName, fieldSchema.getDisplayName()),
            "Supported types are: string, double, boolean, bytes, int, float, long, " +
              "record, array, union and timestamp.").withInputSchemaField(fieldName);
      }
      return;
    }

    switch (fieldSchema.getType()) {
      case STRING:
      case DOUBLE:
      case BOOLEAN:
      case BYTES:
      case INT:
      case FLOAT:
      case LONG:
      case NULL:
        return;
      case RECORD:
        validateSchema(fieldSchema, collector);
        return;
      case ARRAY:
        if (fieldSchema.getComponentSchema() == null) {
          collector.addFailure(String.format("Field '%s' has no schema for array type", fieldName),
            "Ensure array component has schema.").withInputSchemaField(fieldName);
          return;
        }
        Schema componentSchema = fieldSchema.getComponentSchema();
        if (Schema.Type.ARRAY == componentSchema.getType()) {
          collector.addFailure(String.format("Field '%s' is of unsupported type array of array.", fieldName),
            "Ensure the field has valid type.")
            .withInputSchemaField(fieldName);
          return;
        }
        validateSinkFieldSchema(fieldName, componentSchema, collector);
        return;
      case UNION:
        fieldSchema.getUnionSchemas().forEach(unionSchema ->
          validateSinkFieldSchema(fieldName, unionSchema, collector));
        return;
      default:
        collector.addFailure(String.format("Field '%s' is of unsupported type '%s'",
          fieldName, fieldSchema.getDisplayName()),
          "Supported types are: string, double, boolean, bytes, long, record, " +
            "array, union and timestamp.")
          .withInputSchemaField(fieldName);
    }
  }

  /**
   * Validates given input/output schema according the the specified supported types. Fields of types
   * {@link Schema.Type#RECORD}, {@link Schema.Type#ARRAY}, {@link Schema.Type#MAP} will be validated recursively.
   *
   * @param schema                schema to validate.
   * @param supportedLogicalTypes set of supported logical types.
   * @param supportedTypes        set of supported types.
   * @throws IllegalArgumentException in the case when schema is invalid.
   */
  public void validateSchema(Schema schema, Set<Schema.LogicalType> supportedLogicalTypes,
                             Set<Schema.Type> supportedTypes) {

    Preconditions.checkNotNull(supportedLogicalTypes, "Supported logical types can not be null");
    Preconditions.checkNotNull(supportedTypes, "Supported types can not be null");
    if (schema == null) {
      throw new IllegalArgumentException("Schema must be specified");
    }
    Schema nonNullableSchema = schema.isNullable() ? schema.getNonNullable() : schema;
    validateRecordSchema(null, nonNullableSchema, supportedLogicalTypes, supportedTypes);
  }

  private void validateRecordSchema(@Nullable String fieldName, Schema schema,
                                    Set<Schema.LogicalType> supportedLogicalTypes, Set<Schema.Type> supportedTypes) {
    List<Schema.Field> fields = schema.getFields();
    if (fields == null || fields.isEmpty()) {
      throw new IllegalArgumentException("Schema must contain fields");
    }
    for (Schema.Field field : fields) {
      // Use full field name for nested records to construct meaningful errors messages.
      // Full field names will be in the following format: 'record_field_name.nested_record_field_name'
      String fullFieldName = fieldName != null ? String.format("%s.%s", fieldName, field.getName()) :
        field.getName();
      validateFieldSchema(fullFieldName, field.getSchema(), supportedLogicalTypes, supportedTypes);
    }
  }

  private void validateFieldSchema(String fieldName, Schema schema, Set<Schema.LogicalType> supportedLogicalTypes,
                                   Set<Schema.Type> supportedTypes) {
    Schema nonNullableSchema = schema.isNullable() ? schema.getNonNullable() : schema;
    Schema.Type type = nonNullableSchema.getType();
    switch (type) {
      case RECORD:
        validateRecordSchema(fieldName, nonNullableSchema, supportedLogicalTypes, supportedTypes);
        break;
      case ARRAY:
        validateArraySchema(fieldName, nonNullableSchema, supportedLogicalTypes, supportedTypes);
        break;
      case MAP:
        validateMapSchema(fieldName, nonNullableSchema, supportedLogicalTypes, supportedTypes);
        break;
      default:
        validateSchemaType(fieldName, nonNullableSchema, supportedLogicalTypes, supportedTypes);
    }
  }

  private void validateMapSchema(String fieldName, Schema schema, Set<Schema.LogicalType> supportedLogicalTypes,
                                 Set<Schema.Type> supportedTypes) {
    Schema keySchema = schema.getMapSchema().getKey();
    if (keySchema.isNullable()) {
      throw new IllegalArgumentException(String.format(
        "Map keys must be a non-nullable string. Please change field '%s' to be a non-nullable string.",
        fieldName));
    }
    if (keySchema.getType() != Schema.Type.STRING) {
      throw new IllegalArgumentException(String.format(
        "Map keys must be a non-nullable string. Please change field '%s' to be a non-nullable string.",
        fieldName));
    }
    validateFieldSchema(fieldName, schema.getMapSchema().getValue(), supportedLogicalTypes, supportedTypes);
  }

  private void validateArraySchema(String fieldName, Schema schema, Set<Schema.LogicalType> supportedLogicalTypes,
                                   Set<Schema.Type> supportedTypes) {
    Schema componentSchema = schema.getComponentSchema().isNullable() ? schema.getComponentSchema().getNonNullable()
      : schema.getComponentSchema();
    validateFieldSchema(fieldName, componentSchema, supportedLogicalTypes, supportedTypes);
  }

  private void validateSchemaType(String fieldName, Schema fieldSchema, Set<Schema.LogicalType> supportedLogicalTypes,
                                  Set<Schema.Type> supportedTypes) {
    Schema.Type type = fieldSchema.getType();
    Schema.LogicalType logicalType = fieldSchema.getLogicalType();
    if (supportedTypes.contains(type) || supportedLogicalTypes.contains(logicalType)) {
      return;
    }

    String supportedTypeNames = Stream.concat(supportedTypes.stream(), supportedLogicalTypes.stream())
      .map(Enum::name)
      .map(String::toLowerCase)
      .collect(Collectors.joining(", "));

    String actualTypeName = logicalType != null ? logicalType.name().toLowerCase() : type.name().toLowerCase();
    throw new IllegalArgumentException(String.format("Field '%s' is of unsupported type '%s'. " +
      "Supported types are: %s.", fieldName, actualTypeName, supportedTypeNames));
  }

  /**
   * If id type is not auto-generated, validates if id alias column is present in the schema
   * and its type is {@link Schema.Type#STRING}.
   *
   * @param schema    CDAP schema
   * @param collector failure collector
   */
  private void validateIdType(Schema schema, FailureCollector collector) {
    if (containsMacro(PROPERTY_ID_TYPE) || shouldUseAutoGeneratedId(collector)) {
      return;
    }

    Schema.Field field = schema.getField(idAlias);
    if (field == null) {
      collector.addFailure(String.format("Id field '%s' does not exist in the schema", idAlias),
        "Change the Id field to be one of the schema fields.")
        .withConfigProperty(PROPERTY_ID_ALIAS);
      return;
    }

    Schema fieldSchema = field.getSchema();
    Schema.Type type = fieldSchema.getType();
    if (Schema.Type.STRING != type) {
      fieldSchema = fieldSchema.isNullable() ? fieldSchema.getNonNullable() : fieldSchema;
      collector.addFailure(
        String.format("Id field '%s' is of unsupported type '%s'", idAlias, fieldSchema.getDisplayName()),
        "Ensure the type is non-nullable string")
        .withConfigProperty(PROPERTY_ID_ALIAS).withInputSchemaField(idAlias);
    }
  }

  private void validateBatchSize(FailureCollector collector) {
    if (containsMacro(PROPERTY_BATCH_SIZE)) {
      return;
    }
    if (batchSize < 1 || batchSize > MAX_BATCH_SIZE) {
      collector.addFailure(String.format("Invalid Firestore batch size '%d'.", batchSize),
        String.format("Ensure the batch size is at least 1 or at most '%d'", MAX_BATCH_SIZE))
        .withConfigProperty(PROPERTY_BATCH_SIZE);
    }
  }

  /**
   * Returns true if firestore can be connected to or schema is not a macro.
   */
  private boolean shouldConnect() {
    return !containsMacro(NAME_SERVICE_ACCOUNT_FILE_PATH) &&
      !containsMacro(NAME_PROJECT) &&
      tryGetProject() != null &&
      !autoServiceAccountUnavailable();
  }
}
