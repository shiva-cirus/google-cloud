/*
 * Copyright Â© 2019 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package io.cdap.plugin.gcp.firestore.sink;

import com.google.api.core.ApiFuture;
import com.google.cloud.firestore.CollectionReference;
import com.google.cloud.firestore.DocumentReference;
import com.google.cloud.firestore.Firestore;
import com.google.cloud.firestore.WriteBatch;
import com.google.cloud.firestore.WriteResult;
import io.cdap.plugin.gcp.firestore.util.FirestoreUtil;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.RecordWriter;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.List;
import java.util.Map;

import static io.cdap.plugin.gcp.common.GCPConfig.NAME_PROJECT;
import static io.cdap.plugin.gcp.common.GCPConfig.NAME_SERVICE_ACCOUNT_FILE_PATH;
import static io.cdap.plugin.gcp.firestore.sink.util.FirestoreSinkConstants.PROPERTY_BATCH_SIZE;
import static io.cdap.plugin.gcp.firestore.sink.util.FirestoreSinkConstants.PROPERTY_ID_TYPE;
import static io.cdap.plugin.gcp.firestore.util.FirestoreConstants.ID_PROPERTY_NAME;
import static io.cdap.plugin.gcp.firestore.util.FirestoreConstants.PROPERTY_COLLECTION;
import static io.cdap.plugin.gcp.firestore.util.FirestoreConstants.PROPERTY_DATABASE_ID;

/**
 * {@link FirestoreRecordWriter} writes the job outputs to the Firestore.
 */
public class FirestoreRecordWriter extends RecordWriter<NullWritable, Map<String, Object>> {
  private static final Logger LOG = LoggerFactory.getLogger(FirestoreRecordWriter.class);

  private final Firestore db;
  private final int batchSize;
  private final boolean useAutogeneratedId;
  private WriteBatch batch;
  private int totalCount;
  private int numberOfRecordsInBatch;
  private CollectionReference collectionRef;

  public FirestoreRecordWriter(TaskAttemptContext taskAttemptContext) {
    Configuration config = taskAttemptContext.getConfiguration();
    String projectId = config.get(NAME_PROJECT);
    String serviceAccountFilePath = config.get(NAME_SERVICE_ACCOUNT_FILE_PATH);
    String databaseId = config.get(PROPERTY_DATABASE_ID);
    String collection = config.get(PROPERTY_COLLECTION);
    this.batchSize = config.getInt(PROPERTY_BATCH_SIZE, 25);
    this.useAutogeneratedId = config.getBoolean(PROPERTY_ID_TYPE, false);
    LOG.debug("Initialize RecordWriter(projectId={}, database={}, collection={}, serviceFilePath={}, batchSize={}, " +
        "useAutogeneratedId={})", projectId, databaseId, collection, serviceAccountFilePath, batchSize,
      useAutogeneratedId);

    this.db = FirestoreUtil.getFirestore(serviceAccountFilePath, projectId, databaseId);
    this.collectionRef = db.collection(collection);
    this.batch = db.batch();
    this.totalCount = 0;
    this.numberOfRecordsInBatch = 0;
  }

  @Override
  public void write(NullWritable key, Map<String, Object> entity) throws IOException, InterruptedException {
    LOG.trace("RecordWriter write({})", entity);

    /*
    try {
      DocumentReference docRef = collectionRef.document();
      ApiFuture<WriteResult> result = docRef.set(entity);
      result.get();
      ++totalCount;
      ++numberOfRecordsInBatch;
    } catch (Exception e) {
      LOG.error("Error: ", e);
      throw new InterruptedException(e.getMessage());
    }
    */
    DocumentReference docRef = null;
    if (!useAutogeneratedId && entity.containsKey(ID_PROPERTY_NAME)) {
      String documentId = (String) entity.remove(ID_PROPERTY_NAME);
      docRef = collectionRef.document(documentId);
    } else {
      docRef = collectionRef.document();
    }

    batch.set(docRef, entity);

    ++totalCount;
    ++numberOfRecordsInBatch;
    if (totalCount % batchSize == 0) {
      flush();
    }
  }

  @Override
  public void close(TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException {
    flush();
    try {
      db.close();
    } catch (Exception e) {
      throw new IOException(e.getMessage(), e);
    }
    LOG.debug("Total number of values written to Cloud Firestore: {}", totalCount);
  }

  private void flush() throws InterruptedException {
    if (numberOfRecordsInBatch > 0) {
      LOG.debug("Writing a batch of {} values to Cloud Firestore.", numberOfRecordsInBatch);
      try {
        ApiFuture<List<WriteResult>> result = batch.commit();
        result.get();
        batch = db.batch();
        numberOfRecordsInBatch = 0;
      } catch (Exception e) {
        LOG.error("Error while executing batch: ", e);
        throw new InterruptedException(e.getMessage());
      }
    }
  }
}
